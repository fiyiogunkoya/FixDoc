"""Blast radius analysis engine for fixdoc.

Estimates which identities, workloads, and resources are most likely
affected by infrastructure changes before they're applied. Combines
Terraform plan JSON, the terraform graph dependency DAG, and FixDoc's
fix history into a weighted BlastScore (0-100) with explainable
propagation paths.
"""

import re
import uuid
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Optional

from .models import Fix
from .storage import FixRepository

# ---------------------------------------------------------------------------
# Control-point classification
# ---------------------------------------------------------------------------

# Maps resource type prefix -> (category, criticality 0-1)
CONTROL_POINT_PATTERNS: dict[str, tuple[str, float]] = {
    # AWS IAM
    "aws_iam_role_policy_attachment": ("iam", 0.9),
    "aws_iam_role_policy": ("iam", 0.9),
    "aws_iam_policy_attachment": ("iam", 0.9),
    "aws_iam_group_policy_attachment": ("iam", 0.85),
    "aws_iam_user_policy_attachment": ("iam", 0.85),
    "aws_iam_role": ("iam", 0.9),
    "aws_iam_policy": ("iam", 0.85),
    "aws_iam_user": ("iam", 0.8),
    "aws_iam_group": ("iam", 0.75),
    # Azure RBAC
    "azurerm_role_assignment": ("rbac", 0.9),
    "azurerm_key_vault_access_policy": ("rbac", 0.85),
    "azurerm_role_definition": ("rbac", 0.85),
    # GCP IAM
    "google_project_iam": ("iam", 0.9),
    "google_service_account": ("iam", 0.85),
    # Network boundaries
    "aws_security_group": ("network", 0.8),
    "aws_network_acl": ("network", 0.8),
    "aws_route_table": ("network", 0.7),
    "azurerm_network_security_group": ("network", 0.8),
    "azurerm_firewall_rule": ("network", 0.85),
    "google_compute_firewall": ("network", 0.8),
}

# Sensitive value patterns for redaction
SENSITIVE_PATTERNS = re.compile(
    r"(password|secret|token|api_key|private_key|access_key|credentials)",
    re.IGNORECASE,
)

# Recommended checks per control-point category
CATEGORY_CHECKS: dict[str, list[str]] = {
    "iam": [
        "Review IAM policy least-privilege before applying",
        "Check service account permissions",
    ],
    "rbac": [
        "Review RBAC role assignment scope",
        "Verify key vault access policy changes",
    ],
    "network": [
        "Verify security group rules",
        "Check for open 0.0.0.0/0 rules",
    ],
}

DELETE_CHECKS = ["Confirm resource is not referenced by other stacks"]

# Category tags that indicate a fix is relevant to a specific concern domain.
# Resource-type-only tags (e.g. "aws_instance") are NOT sufficient — a fix must
# have at least one of these to surface in Phase 2 history matching.
_HISTORY_CATEGORY_TAGS: frozenset = frozenset({
    "networking", "network", "rbac", "iam", "dns", "quota", "state",
    "state-lock", "auth", "authentication", "authorization", "acl",
    "route", "routing", "connectivity", "ingress", "egress", "security",
    "firewall", "k8s", "kubernetes", "key_vault", "vault", "certificate",
    "cert", "database", "db", "storage", "permissions",
})

# Action points for the linear scoring formula
ACTION_POINTS = {
    "delete": 20,
    "replace": 25,
    "update": 5,
    "create": 8,
}

# Discount multipliers for all-create (greenfield) plans.
# Creating new resources poses lower risk than modifying existing ones.
GREENFIELD_MULTIPLIER = 0.3           # non-boundary creates
GREENFIELD_BOUNDARY_MULTIPLIER = 0.5  # boundary creates (smaller discount — still risky if misconfigured)
GREENFIELD_IMPACT_MULTIPLIER = 0.25   # fraction of normal L1/L2 weight for cross-boundary edges


# ---------------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------------


@dataclass
class BlastNode:
    """A resource node in the blast radius graph."""

    address: str
    resource_type: str
    action: str  # create, update, delete, replace, no-op
    cloud_provider: str = "unknown"
    is_control_point: bool = False
    criticality: float = 0.0
    category: str = ""


@dataclass
class AffectedResource:
    """A resource reached by BFS propagation from a control point."""

    address: str
    resource_type: str
    depth: int
    path: list[str] = field(default_factory=list)


@dataclass
class BlastResult:
    """Complete result of a blast radius analysis."""

    analysis_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    timestamp: str = field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    score: float = 0.0
    severity: str = "low"
    changes: list[dict] = field(default_factory=list)
    control_points: list[dict] = field(default_factory=list)
    affected: list[dict] = field(default_factory=list)
    why_paths: list[dict] = field(default_factory=list)
    checks: list[str] = field(default_factory=list)
    history_matches: list[dict] = field(default_factory=list)
    plan_summary: dict = field(default_factory=dict)


# ---------------------------------------------------------------------------
# Control-point classification
# ---------------------------------------------------------------------------


def classify_control_point(resource_type: str) -> Optional[tuple[str, float]]:
    """Classify a resource type as a control point.

    Uses prefix matching so e.g. 'google_project_iam_member' matches
    'google_project_iam'.

    Returns (category, criticality) or None if not a control point.
    """
    rt_lower = resource_type.lower()
    # Try longest prefix first so more specific patterns win
    best_match = None
    best_len = 0
    for prefix, (category, criticality) in CONTROL_POINT_PATTERNS.items():
        if rt_lower.startswith(prefix) and len(prefix) > best_len:
            best_match = (category, criticality)
            best_len = len(prefix)
    return best_match


def is_boundary_resource(resource_type: str) -> bool:
    """Check if a resource type is a boundary/control-point resource."""
    return classify_control_point(resource_type) is not None


# ---------------------------------------------------------------------------
# DOT graph parser
# ---------------------------------------------------------------------------

# Matches edges: "node_a" -> "node_b" or unquoted node_a -> node_b
_EDGE_RE = re.compile(
    r'"([^"]+)"\s*->\s*"([^"]+)"'
    r"|"
    r"(\S+)\s*->\s*(\S+)"
)


def _normalize_tf_node(name: str) -> str:
    """Normalize a Terraform graph node name to match plan addresses.

    Strips '[root] ' prefix and '(expand)'/'(close)' suffixes.
    """
    name = name.strip()
    if name.startswith("[root] "):
        name = name[7:]
    name = re.sub(r"\s*\(expand\)\s*$", "", name)
    name = re.sub(r"\s*\(close\)\s*$", "", name)
    return name.strip()


def parse_dot_graph(dot_text: str) -> tuple[dict[str, set[str]], dict[str, set[str]]]:
    """Parse a Terraform DOT graph into forward and reverse adjacency lists.

    Returns (forward_adj, reverse_adj) where forward means A -> B
    (A depends on B or B is downstream of A, depending on TF graph direction).
    """
    forward: dict[str, set[str]] = {}
    reverse: dict[str, set[str]] = {}

    for line in dot_text.splitlines():
        stripped = line.strip()
        # Skip comments, subgraph declarations, closing braces
        if (
            stripped.startswith("//")
            or stripped.startswith("#")
            or stripped.startswith("subgraph")
            or stripped.startswith("digraph")
            or stripped in ("}", "{")
        ):
            continue

        m = _EDGE_RE.search(stripped)
        if m:
            if m.group(1) is not None:
                src = _normalize_tf_node(m.group(1))
                dst = _normalize_tf_node(m.group(2))
            else:
                src = _normalize_tf_node(m.group(3))
                dst = _normalize_tf_node(m.group(4))

            forward.setdefault(src, set()).add(dst)
            forward.setdefault(dst, set())
            reverse.setdefault(dst, set()).add(src)
            reverse.setdefault(src, set())

    return forward, reverse


# ---------------------------------------------------------------------------
# Bounded BFS
# ---------------------------------------------------------------------------


def compute_affected_set(
    start_nodes: list[str],
    adjacency: dict[str, set[str]],
    max_depth: int = 5,
) -> list[AffectedResource]:
    """BFS from start_nodes through adjacency, bounded by max_depth.

    Returns list of AffectedResource with traversal paths.
    Keeps shortest path when reached from multiple starts.
    """
    visited: dict[str, AffectedResource] = {}
    queue: deque[tuple[str, int, list[str]]] = deque()

    for node in start_nodes:
        queue.append((node, 0, [node]))
        visited[node] = AffectedResource(
            address=node, resource_type="", depth=0, path=[node]
        )

    while queue:
        current, depth, path = queue.popleft()
        if depth >= max_depth:
            continue

        for neighbor in adjacency.get(current, set()):
            if neighbor not in visited:
                new_path = path + [neighbor]
                visited[neighbor] = AffectedResource(
                    address=neighbor,
                    resource_type="",
                    depth=depth + 1,
                    path=new_path,
                )
                queue.append((neighbor, depth + 1, new_path))

    # Remove start nodes from results (they are the changes, not affected)
    return [ar for ar in visited.values() if ar.address not in start_nodes]


def compute_tiered_affected(
    changed_nodes: list[BlastNode],
    adjacency: dict[str, set[str]],
    max_depth: int = 5,
) -> tuple[list[AffectedResource], list[AffectedResource]]:
    """Compute tiered affected sets: L1 (direct) and L2 (indirect).

    L2 is only populated if any L0 node is a boundary resource or
    involves a delete/replace action.

    Returns (l1_affected, l2_affected).
    """
    start_addrs = [n.address for n in changed_nodes]

    # Always compute L1 (depth 1)
    all_affected = compute_affected_set(start_addrs, adjacency, max_depth=max_depth)
    l1 = [ar for ar in all_affected if ar.depth == 1]
    l2 = [ar for ar in all_affected if ar.depth >= 2]

    # Gate L2: only include if L0 has boundary resources or delete/replace
    has_boundary = any(is_boundary_resource(n.resource_type) for n in changed_nodes)
    has_destructive = any(n.action in ("delete", "replace") for n in changed_nodes)

    if not (has_boundary or has_destructive):
        l2 = []

    return l1, l2


# ---------------------------------------------------------------------------
# Blast score formula — linear
# ---------------------------------------------------------------------------


def _normalize_action(actions: list[str]) -> str:
    """Normalize a Terraform actions list to a single action string.

    Treats ["create", "delete"] as "replace".
    """
    action_set = set(actions)
    if "create" in action_set and "delete" in action_set:
        return "replace"
    if "delete" in action_set:
        return "delete"
    if "update" in action_set:
        return "update"
    if "create" in action_set:
        return "create"
    return "no-op"


def compute_blast_score(
    changed_nodes: list[BlastNode],
    l1_count: int,
    l2_count: int,
    history_match_count: int,
) -> float:
    """Compute blast score 0-100 using linear formula.

    Score components:
    1. Action points for each changed (L0) resource
    2. Impact points from dependents (L1 + L2)
    3. History overlay

    For greenfield plans (all creates):
    - Non-boundary creates: GREENFIELD_MULTIPLIER (0.3x)
    - Boundary creates: GREENFIELD_BOUNDARY_MULTIPLIER (0.5x) — smaller discount
    - L1/L2: caller pre-filters to cross-boundary existing-infra edges only,
      weighted at 1.5 * GREENFIELD_IMPACT_MULTIPLIER (0.375x normal)
    """
    score = 0.0

    # No-ops/reads are already excluded by analyze_blast_radius() before this call.
    # Greenfield: all active changes are creates.
    is_greenfield = bool(changed_nodes) and all(
        node.action == "create" for node in changed_nodes
    )

    # 1. Action points for each L0 resource
    all_updates_no_boundary = True
    action_points = 0.0

    for node in changed_nodes:
        points = ACTION_POINTS.get(node.action, 0)
        if is_boundary_resource(node.resource_type):
            points *= 1.5
            if is_greenfield:
                points *= GREENFIELD_BOUNDARY_MULTIPLIER
        else:
            if is_greenfield:
                points *= GREENFIELD_MULTIPLIER
        if node.action != "update" or is_boundary_resource(node.resource_type):
            all_updates_no_boundary = False
        action_points += points

    score += action_points

    # 2. Impact points from dependents
    # For greenfield: l1_count/l2_count are pre-filtered by analyze_blast_radius()
    # to only count resources NOT in the plan (cross-boundary existing-infra edges).
    impacted_count = min(l1_count + l2_count, 25)
    if is_greenfield:
        # New resources touching existing infra: low weight (0.25 × normal 1.5)
        impact_multiplier = 1.5 * GREENFIELD_IMPACT_MULTIPLIER
    elif all_updates_no_boundary:
        impact_multiplier = 0.5
    else:
        impact_multiplier = 1.5
    score += impacted_count * impact_multiplier

    # 3. History overlay
    score += min(history_match_count * 5, 15)

    # Clamp
    score = min(100.0, max(0.0, score))
    return round(score, 1)


def severity_label(score: float) -> str:
    """Map blast score to severity label."""
    if score >= 75:
        return "critical"
    if score >= 50:
        return "high"
    if score >= 25:
        return "medium"
    return "low"


# ---------------------------------------------------------------------------
# History-prior scoring
# ---------------------------------------------------------------------------


def _history_cluster_key(fix: Fix) -> str:
    """Return dedup cluster key: first CamelCase error token, or first 4 words."""
    issue = fix.issue or ""
    m = re.search(r'[A-Z][a-z]+(?:[A-Z][a-z]+)+', issue)
    if m:
        return m.group()
    words = re.sub(r'[^\w\s]', '', issue).lower().split()[:4]
    return ' '.join(words)


def _dedup_history_candidates(
    candidates: list[tuple[Fix, str]]
) -> list[tuple[Fix, str]]:
    """Cluster by error fingerprint; keep most-complete fix per cluster.

    Most complete = has error_excerpt, then most recent created_at.
    """
    clusters: dict[str, tuple[Fix, str]] = {}
    for fix, rt in candidates:
        key = _history_cluster_key(fix)
        if key not in clusters:
            clusters[key] = (fix, rt)
        else:
            existing_fix, _ = clusters[key]
            existing_score = (1 if existing_fix.error_excerpt else 0, existing_fix.created_at)
            new_score = (1 if fix.error_excerpt else 0, fix.created_at)
            if new_score > existing_score:
                clusters[key] = (fix, rt)
    return list(clusters.values())


def compute_history_prior(
    changed_resource_types: list[str],
    changed_nodes: list[BlastNode],
    repo: FixRepository,
) -> tuple[int, list[dict]]:
    """Compute history match count from fix database.

    Returns (match_count, list_of_matching_fix_dicts).
    Only returns matches when:
    - A changed resource is a control point (boundary), OR
    - Any action is delete/replace, OR
    - A fix's issue/error_excerpt mentions a changed resource address exactly.
    Matches are category-tag filtered, deduped, and capped at 3.
    """
    has_boundary = any(is_boundary_resource(n.resource_type) for n in changed_nodes)
    has_destructive = any(n.action in ("delete", "replace") for n in changed_nodes)
    changed_addresses = {n.address.lower() for n in changed_nodes}

    seen_ids: set[str] = set()
    candidates: list[tuple[Fix, str]] = []  # (fix, resource_type)

    # Phase 1: Address-match override — works even without gate
    for fix in repo.list_all():
        fix_searchable = " ".join(filter(None, [fix.issue, fix.error_excerpt])).lower()
        if any(addr in fix_searchable for addr in changed_addresses):
            if fix.id not in seen_ids:
                seen_ids.add(fix.id)
                candidates.append((fix, ""))

    # Phase 2: Resource-type + category tag — only when gate passes
    if has_boundary or has_destructive:
        for rt in changed_resource_types:
            for fix in repo.find_by_resource_type(rt):
                if fix.id in seen_ids:
                    continue
                if not fix.tags:
                    continue
                fix_tags = {t.strip().lower() for t in fix.tags.split(",") if t.strip()}
                if fix_tags & _HISTORY_CATEGORY_TAGS:
                    seen_ids.add(fix.id)
                    candidates.append((fix, rt))

    if not candidates:
        return 0, []

    # Dedup and cap at 3
    deduped = _dedup_history_candidates(candidates)[:3]

    result = [
        {"id": fix.id[:8], "issue": fix.issue, "resource_type": rt}
        for fix, rt in deduped
    ]
    return len(result), result


# ---------------------------------------------------------------------------
# Redaction
# ---------------------------------------------------------------------------


def redact_plan_values(change_block: dict) -> dict:
    """Redact sensitive values from a plan change block.

    1. Honors sensitive_values markers from the plan JSON.
    2. Pattern-matches keys against SENSITIVE_PATTERNS.
    3. Replaces values with '[REDACTED]'.
    """
    result = {}
    sensitive_keys = set()

    # Collect keys flagged by Terraform's sensitive_values
    for phase in ("before_sensitive", "after_sensitive"):
        sv = change_block.get(phase)
        if isinstance(sv, dict):
            _collect_sensitive_keys(sv, "", sensitive_keys)

    for key, value in change_block.items():
        if key in ("before_sensitive", "after_sensitive"):
            continue
        if isinstance(value, dict):
            result[key] = _redact_dict(value, sensitive_keys, "")
        else:
            result[key] = value

    return result


def _collect_sensitive_keys(
    sensitive_map: dict, prefix: str, out: set[str]
) -> None:
    """Recursively collect keys marked as sensitive."""
    for key, val in sensitive_map.items():
        full_key = f"{prefix}.{key}" if prefix else key
        if val is True:
            out.add(full_key)
        elif isinstance(val, dict):
            _collect_sensitive_keys(val, full_key, out)


def _redact_dict(
    d: dict, sensitive_keys: set[str], prefix: str
) -> dict:
    """Recursively redact sensitive values in a dict."""
    result = {}
    for key, val in d.items():
        full_key = f"{prefix}.{key}" if prefix else key
        if full_key in sensitive_keys or SENSITIVE_PATTERNS.search(key):
            result[key] = "[REDACTED]"
        elif isinstance(val, dict):
            result[key] = _redact_dict(val, sensitive_keys, full_key)
        else:
            result[key] = val
    return result


# ---------------------------------------------------------------------------
# Recommended checks generator
# ---------------------------------------------------------------------------


def generate_checks(
    control_points: list[BlastNode],
    has_deletes: bool,
) -> list[str]:
    """Generate recommended checks based on control point categories."""
    checks: list[str] = []
    seen_categories: set[str] = set()

    for cp in control_points:
        if cp.category and cp.category not in seen_categories:
            seen_categories.add(cp.category)
            checks.extend(CATEGORY_CHECKS.get(cp.category, []))

    if has_deletes:
        checks.extend(DELETE_CHECKS)

    return checks


# ---------------------------------------------------------------------------
# Main analysis orchestrator
# ---------------------------------------------------------------------------


def analyze_blast_radius(
    plan: dict,
    repo: FixRepository,
    dot_text: Optional[str] = None,
    max_depth: int = 5,
) -> BlastResult:
    """Run a full blast radius analysis on a Terraform plan.

    Args:
        plan: Parsed Terraform plan JSON.
        repo: FixRepository for history lookup.
        dot_text: Optional DOT graph text from `terraform graph`.
        max_depth: Max BFS traversal depth.

    Returns:
        BlastResult with score, severity, affected resources, etc.
    """
    from .commands.analyze import TerraformAnalyzer

    analyzer = TerraformAnalyzer(repo=repo)
    resources = analyzer.extract_resources(plan)

    # Build nodes and identify control points — changed only
    nodes: list[BlastNode] = []
    control_points: list[BlastNode] = []
    changes: list[dict] = []

    for res in resources:
        action = res.action
        if action in ("no-op", "read", "refresh-only", "unknown"):
            continue

        # Detect replace: ["create", "delete"]
        # The action is already normalized by TerraformAnalyzer, but
        # we check the raw plan for the create+delete combo
        raw_actions = []
        for rc in plan.get("resource_changes", []):
            if rc.get("address") == res.address:
                raw_actions = rc.get("change", {}).get("actions", [])
                break
        if "create" in raw_actions and "delete" in raw_actions:
            action = "replace"

        cp_info = classify_control_point(res.resource_type)
        is_cp = cp_info is not None
        category = cp_info[0] if cp_info else ""
        criticality = cp_info[1] if cp_info else 0.0

        node = BlastNode(
            address=res.address,
            resource_type=res.resource_type,
            action=action,
            cloud_provider=res.cloud_provider.value,
            is_control_point=is_cp,
            criticality=criticality,
            category=category,
        )
        nodes.append(node)

        if is_cp:
            control_points.append(node)

        changes.append(
            {
                "address": res.address,
                "resource_type": res.resource_type,
                "action": action,
                "cloud_provider": res.cloud_provider.value,
                "is_control_point": is_cp,
                "category": category,
                "criticality": criticality,
            }
        )

    # BFS propagation if graph is available
    l1_affected: list[AffectedResource] = []
    l2_affected: list[AffectedResource] = []
    all_affected: list[AffectedResource] = []

    if dot_text and nodes:
        _forward, reverse = parse_dot_graph(dot_text)
        # Use reverse adjacency only: reverse[X] = things that depend on X.
        # This ensures BFS traverses downstream dependents, not upstream
        # dependencies (subnet, VPC) that are not impacted by a change to X.
        l1_affected, l2_affected = compute_tiered_affected(
            nodes, reverse, max_depth
        )
        all_affected = l1_affected + l2_affected

    # History prior
    changed_types = list({n.resource_type for n in nodes})
    history_count, history_matches = compute_history_prior(changed_types, nodes, repo)

    # Blast score — linear formula.
    # Filter L1/L2 to only count resources NOT in the plan itself, so that
    # intra-plan dependency edges (new resource → new resource) don't inflate
    # the score. For greenfield plans this removes all intra-plan L1/L2 noise;
    # only cross-boundary edges to existing infra are counted (at reduced weight).
    changed_addresses = {n.address for n in nodes}
    l1_score_count = sum(1 for ar in l1_affected if ar.address not in changed_addresses)
    l2_score_count = sum(1 for ar in l2_affected if ar.address not in changed_addresses)

    score = compute_blast_score(
        nodes, l1_score_count, l2_score_count, history_count
    )
    sev = severity_label(score)

    # Recommended checks
    has_deletes = any(n.action in ("delete", "replace") for n in nodes)
    checks = generate_checks(control_points, has_deletes)

    # Build why-paths for affected resources
    why_paths = []
    for ar in all_affected[:20]:  # Cap at 20 for readability
        why_paths.append(
            {
                "target": ar.address,
                "depth": ar.depth,
                "path": ar.path,
            }
        )

    # Plan summary
    plan_summary = {
        "total_changes": len(nodes),
        "control_points": len(control_points),
        "affected_resources": len(all_affected),
        "by_action": {},
    }
    for n in nodes:
        plan_summary["by_action"][n.action] = (
            plan_summary["by_action"].get(n.action, 0) + 1
        )

    return BlastResult(
        score=score,
        severity=sev,
        changes=changes,
        control_points=[
            {
                "address": cp.address,
                "resource_type": cp.resource_type,
                "action": cp.action,
                "category": cp.category,
                "criticality": cp.criticality,
            }
            for cp in control_points
        ],
        affected=[
            {
                "address": ar.address,
                "depth": ar.depth,
                "path": ar.path,
            }
            for ar in all_affected
        ],
        why_paths=why_paths,
        checks=checks,
        history_matches=history_matches,
        plan_summary=plan_summary,
    )


def _find_change_block(plan: dict, address: str) -> Optional[dict]:
    """Find the change block for a resource address in the plan."""
    for change in plan.get("resource_changes", []):
        if change.get("address") == address:
            return change.get("change", {})
    return None
